from __future__ import annotations

import argparse
import logging
from datetime import datetime, timedelta, timezone
from pathlib import Path

import config
from fetcher import Article, fetch_all_feeds
from filter import filter_articles
from notifier import post_message
from storage import load_sent_urls, save_sent_urls
from extrasources import fetch_medicaltech, fetch_htwatch, fetch_google_news

logger = logging.getLogger(__name__)


def _count_keyword_hits(articles: list[Article]) -> tuple[int, int, int]:
    med_kw = [kw.lower() for kw in config.MEDICAL_KEYWORDS]
    it_kw = [kw.lower() for kw in config.IT_KEYWORDS]
    med_count = it_count = both_count = 0
    for a in articles:
        text = f"{a.title} {a.summary or ''}".lower()
        has_med = any(kw in text for kw in med_kw)
        has_it = any(kw in text for kw in it_kw)
        med_count += has_med
        it_count += has_it
        both_count += has_med and has_it
    return med_count, it_count, both_count


def _get_source_name(link: str) -> str:
    """URLã‹ã‚‰ã‚½ãƒ¼ã‚¹åã‚’å–å¾—ã™ã‚‹ã€‚"""
    if "prtimes.jp" in link:
        return "PR TIMES"
    elif "news.google.com" in link:
        return "Google News"
    elif "medicaltech-news.com" in link:
        return "åŒ»ç™‚ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹"
    elif "ht-watch.com" in link:
        return "ãƒ˜ãƒ«ã‚¹ãƒ†ãƒƒã‚¯ã‚¦ã‚©ãƒƒãƒ"
    else:
        return "ãã®ä»–"


def _categorize_keywords(keywords: list[str], medical_keywords: list[str], it_keywords: list[str]) -> tuple[list[str], list[str]]:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åŒ»ç™‚ç³»ã¨ITç³»ã«åˆ†é¡ã™ã‚‹ã€‚"""
    med_kw_lower = [kw.lower() for kw in medical_keywords]
    it_kw_lower = [kw.lower() for kw in it_keywords]
    
    medical_matched = []
    it_matched = []
    
    for kw in keywords:
        kw_lower = kw.lower()
        if kw_lower in med_kw_lower:
            medical_matched.append(kw)
        if kw_lower in it_kw_lower:
            it_matched.append(kw)
    
    return medical_matched, it_matched


def build_message(articles: list[Article], now: datetime) -> str:
    if not articles:
        return "ğŸ©ºğŸ¤– æœ¬æ™‚é–“å¸¯ã®åŒ»ç™‚Ã—ITæ–°ç€ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    header = f"ğŸ©ºğŸ¤– *åŒ»ç™‚Ã—ITãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚*ï¼ˆ{now.strftime('%Y-%m-%d %H:%M JST')}ï¼‰"
    lines = [header, ""]
    
    # ã‚½ãƒ¼ã‚¹åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    from collections import defaultdict
    articles_by_source = defaultdict(list)
    for article in articles:
        source = _get_source_name(article.link)
        articles_by_source[source].append(article)
    
    # è¨˜äº‹ã‚’ã‚½ãƒ¼ã‚¹åˆ¥ã«è¡¨ç¤ºï¼ˆå„ªå…ˆåº¦é †ï¼‰
    idx = 1
    # ã‚½ãƒ¼ã‚¹ã®å„ªå…ˆåº¦é †ï¼ˆPR TIMESãŒæœ€å„ªå…ˆã€ã€Œãã®ä»–ã€ãŒæœ€ä¸‹ä½ï¼‰
    source_priority = {
        "PR TIMES": 1,
        "åŒ»ç™‚ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹": 2,
        "ãƒ˜ãƒ«ã‚¹ãƒ†ãƒƒã‚¯ã‚¦ã‚©ãƒƒãƒ": 3,
        "Google News": 4,
        "ãã®ä»–": 5,
    }
    
    for source in sorted(articles_by_source.keys(), key=lambda s: source_priority.get(s, 99)):
        source_articles = articles_by_source[source]
        lines.append(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append(f"ğŸ“° *{source}* ({len(source_articles)}ä»¶)")
        lines.append("")
        
        for article in source_articles:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒã‚¤ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯å½¢å¼ã«ã™ã‚‹
            lines.append(f"*{idx}. <{article.link}|{article.title}>*")
            
            # æ¦‚è¦ï¼ˆdescriptionï¼‰ãŒã‚ã‚Œã°è¡¨ç¤ºï¼ˆæœ€å¤§150æ–‡å­—ã€æ–‡ã®é€”ä¸­ã§åˆ‡ã‚‰ãªã„ï¼‰
            if article.summary:
                summary = article.summary.strip()
                if len(summary) > 150:
                    # 150æ–‡å­—ä»¥å†…ã§æ–‡ã®çµ‚ã‚ã‚Šï¼ˆå¥ç‚¹ã€æ”¹è¡Œï¼‰ã‚’æ¢ã™
                    truncated = summary[:150]
                    # æœ€å¾Œã®å¥ç‚¹ã€æ”¹è¡Œã€ã¾ãŸã¯é©åˆ‡ãªåŒºåˆ‡ã‚Šä½ç½®ã‚’æ¢ã™
                    for delimiter in ['ã€‚', '\n', '.', 'ï¼', 'ï¼Ÿ']:
                        last_pos = truncated.rfind(delimiter)
                        if last_pos > 100:  # 100æ–‡å­—ä»¥ä¸Šã¯ç¢ºä¿
                            truncated = truncated[:last_pos + 1]
                            break
                    summary = truncated + "..."
                lines.append(f"   {summary}")
            
            idx += 1
            if idx <= len(articles):  # æœ€å¾Œã®è¨˜äº‹ä»¥å¤–ã¯ç©ºè¡Œã‚’è¿½åŠ 
                lines.append("")
    
    lines.append("")
    footer = f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ“Š åˆè¨ˆ *{len(articles)}ä»¶*ã®è¨˜äº‹ã‚’é…ä¿¡"
    lines.append(footer)
    
    return "\n".join(lines).rstrip()


def _normalize_title(title: str) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒç”¨ã«ã™ã‚‹ã€‚"""
    import re
    # è¨˜å·ã€ç©ºç™½ã€æ”¹è¡Œã‚’å‰Šé™¤ã—ã¦å°æ–‡å­—ã«å¤‰æ›
    normalized = re.sub(r'[^\w]', '', title.lower())
    return normalized


def _calculate_title_similarity(title1: str, title2: str) -> float:
    """2ã¤ã®ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆ0.0-1.0ï¼‰ã€‚"""
    norm1 = _normalize_title(title1)
    norm2 = _normalize_title(title2)
    
    if not norm1 or not norm2:
        return 0.0
    
    # å®Œå…¨ä¸€è‡´
    if norm1 == norm2:
        return 1.0
    
    # çŸ­ã„æ–¹ãŒé•·ã„æ–¹ã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
    shorter = min(len(norm1), len(norm2))
    longer = max(len(norm1), len(norm2))
    
    if shorter >= 10:  # 10æ–‡å­—ä»¥ä¸Šã®å ´åˆã®ã¿éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
        if len(norm1) <= len(norm2):
            if norm1 in norm2:
                # éƒ¨åˆ†ä¸€è‡´ã®å ´åˆã¯ã€çŸ­ã„æ–¹ã®é•·ã• / é•·ã„æ–¹ã®é•·ã•ã§é¡ä¼¼åº¦ã‚’è¨ˆç®—
                # ãŸã ã—ã€æœ€ä½ã§ã‚‚0.7ä»¥ä¸Šã«ã™ã‚‹ï¼ˆçŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«ãŒé•·ã„ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸€éƒ¨ãªã‚‰é«˜é¡ä¼¼åº¦ï¼‰
                base_similarity = len(norm1) / len(norm2)
                return max(base_similarity, 0.7)
        else:
            if norm2 in norm1:
                base_similarity = len(norm2) / len(norm1)
                return max(base_similarity, 0.7)
    
    # å…ˆé ­éƒ¨åˆ†ã®ä¸€è‡´ã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆçŸ­ã„æ–¹ã®80%ä»¥ä¸ŠãŒé•·ã„æ–¹ã®å…ˆé ­ã¨ä¸€è‡´ã™ã‚‹å ´åˆï¼‰
    if shorter >= 5:
        overlap_length = min(shorter, int(shorter * 0.8))
        if len(norm1) <= len(norm2):
            if norm1[:overlap_length] == norm2[:overlap_length]:
                return 0.75  # å…ˆé ­ãŒä¸€è‡´ã—ã¦ã„ã‚‹å ´åˆã¯é«˜é¡ä¼¼åº¦
        else:
            if norm2[:overlap_length] == norm1[:overlap_length]:
                return 0.75
    
    # å…±é€šæ–‡å­—æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆé †åºã¯è€ƒæ…®ã—ãªã„ï¼‰
    common_chars = sum(1 for c in set(norm1) if c in norm2)
    total_chars = len(set(norm1) | set(norm2))
    
    if total_chars == 0:
        return 0.0
    
    # Jaccardé¡ä¼¼åº¦ï¼ˆæ–‡å­—é›†åˆã®é¡ä¼¼åº¦ï¼‰
    jaccard_similarity = common_chars / total_chars
    
    # é•·ã•ã®é¡ä¼¼åº¦ã‚‚è€ƒæ…®
    shorter = min(len(norm1), len(norm2))
    longer = max(len(norm1), len(norm2))
    length_similarity = shorter / longer if longer > 0 else 0.0
    
    # å…±é€šéƒ¨åˆ†æ–‡å­—åˆ—ã®é•·ã•ã‚’è€ƒæ…®ï¼ˆæœ€é•·å…±é€šéƒ¨åˆ†åˆ—ã®ç°¡æ˜“ç‰ˆï¼‰
    # 3æ–‡å­—ä»¥ä¸Šã®å…±é€šéƒ¨åˆ†æ–‡å­—åˆ—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    common_substring_score = 0.0
    for i in range(len(norm1) - 2):
        substring = norm1[i:i+3]
        if substring in norm2:
            common_substring_score = 0.2
            break
    
    # é‡ã¿ä»˜ãå¹³å‡
    similarity = (jaccard_similarity * 0.5) + (length_similarity * 0.3) + common_substring_score
    
    return min(similarity, 1.0)  # 1.0ã‚’è¶…ãˆãªã„ã‚ˆã†ã«ã™ã‚‹


def _get_source_priority(link: str) -> int:
    """ã‚½ãƒ¼ã‚¹ã®å„ªå…ˆåº¦ã‚’è¿”ã™ï¼ˆæ•°å€¤ãŒå°ã•ã„ã»ã©å„ªå…ˆåº¦ãŒé«˜ã„ï¼‰ã€‚"""
    if "prtimes.jp" in link:
        return 1  # PR TIMESãŒæœ€å„ªå…ˆ
    elif "medicaltech-news.com" in link:
        return 2
    elif "ht-watch.com" in link:
        return 3
    elif "news.google.com" in link:
        return 4  # Google Newsã¯ã€Œãã®ä»–ã€ã‚ˆã‚Šå„ªå…ˆ
    else:
        return 5  # ãã®ä»–ãŒæœ€ä¸‹ä½


def deduplicate_articles(articles: list[Article]) -> list[Article]:
    """URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã®é‡è¤‡ã‚’é™¤å»ã™ã‚‹ï¼ˆPR TIMESã‚’æœ€å„ªå…ˆï¼‰ã€‚"""
    # ã¾ãšURLã§é‡è¤‡ã‚’é™¤å»
    seen_urls: set[str] = set()
    url_deduplicated: list[Article] = []
    for article in articles:
        if not article.link:
            continue
        # URLã‚’æ­£è¦åŒ–ï¼ˆæœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚„ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è€ƒæ…®ï¼‰
        normalized_url = article.link.rstrip("/").split("?")[0]
        if normalized_url not in seen_urls:
            seen_urls.add(normalized_url)
            url_deduplicated.append(article)
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã®é‡è¤‡ã‚’é™¤å»ï¼ˆå„ªå…ˆåº¦ã®é«˜ã„ã‚½ãƒ¼ã‚¹ã‚’æ®‹ã™ï¼‰
    # é¡ä¼¼åº¦ã®é–¾å€¤ï¼ˆ0.75ä»¥ä¸Šã§é‡è¤‡ã¨ã¿ãªã™ï¼‰
    SIMILARITY_THRESHOLD = 0.75
    
    deduplicated: list[Article] = []
    for article in url_deduplicated:
        if not article.title:
            continue
        
        # æ—¢ã«è¿½åŠ ã•ã‚ŒãŸè¨˜äº‹ã¨é¡ä¼¼åº¦ã‚’ãƒã‚§ãƒƒã‚¯
        is_duplicate = False
        for existing_article in deduplicated:
            similarity = _calculate_title_similarity(article.title, existing_article.title)
            if similarity >= SIMILARITY_THRESHOLD:
                # é‡è¤‡ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆã€å„ªå…ˆåº¦ã®é«˜ã„æ–¹ã‚’æ®‹ã™
                existing_priority = _get_source_priority(existing_article.link)
                new_priority = _get_source_priority(article.link)
                
                if new_priority < existing_priority:
                    # æ–°ã—ã„è¨˜äº‹ã®æ–¹ãŒå„ªå…ˆåº¦ãŒé«˜ã„å ´åˆã€æ—¢å­˜ã®è¨˜äº‹ã‚’ç½®ãæ›ãˆ
                    deduplicated.remove(existing_article)
                    deduplicated.append(article)
                    logger.debug(
                        "Replaced duplicate article (similarity: %.2f, kept from %s): %s",
                        similarity,
                        _get_source_name(article.link),
                        article.title[:50],
                    )
                else:
                    # æ—¢å­˜ã®è¨˜äº‹ã®æ–¹ãŒå„ªå…ˆåº¦ãŒé«˜ã„å ´åˆã€æ–°ã—ã„è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    logger.debug(
                        "Skipped duplicate article (similarity: %.2f, kept from %s): %s",
                        similarity,
                        _get_source_name(existing_article.link),
                        article.title[:50],
                    )
                is_duplicate = True
                break
        
        if not is_duplicate:
            deduplicated.append(article)
    
    removed_count = len(articles) - len(deduplicated)
    if removed_count > 0:
        logger.info("Removed %d duplicate articles (by URL and title)", removed_count)
    return deduplicated


def sort_articles(articles: list[Article]) -> list[Article]:
    return sorted(
        articles,
        key=lambda a: a.published_at.timestamp() if a.published_at else float("-inf"),
        reverse=True,
    )


def run(dry_run: bool = False, storage_path: Path | None = None, max_items: int | None = None, manual: bool = False) -> int:
    storage_path = storage_path or config.SENT_URLS_PATH
    # æ‰‹å‹•å®Ÿè¡Œæ™‚ã¯5ä»¶ã«åˆ¶é™ã€ãã‚Œä»¥å¤–ã¯æŒ‡å®šã•ã‚ŒãŸmax_itemsã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    if manual:
        max_items = 5
    else:
        max_items = max_items or config.MAX_ARTICLES_PER_POST

    fetched: list[Article] = []
    
    # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    feed_urls = config.RSS_FEEDS
    if feed_urls:
        logger.info("Starting fetch for %d RSS feed(s)", len(feed_urls))
        rss_articles = fetch_all_feeds(feed_urls, timeout=config.FETCH_TIMEOUT)
        if not rss_articles:
            logger.warning("RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            med_count, it_count, both_count = _count_keyword_hits(rss_articles)
            logger.info("RSS keyword hits (before exclude): med=%d it=%d both=%d", med_count, it_count, both_count)
            fetched.extend(rss_articles)
    else:
        logger.info("RSS_FEEDS ãŒç©ºã®ãŸã‚ã€RSSå–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    # Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚½ãƒ¼ã‚¹ï¼ˆå„ªå…ˆï¼‰
    if config.EXTRA_SOURCES:
        logger.info("Fetching web scraping sources: %s", ", ".join(config.EXTRA_SOURCES))
    extra_articles: list[Article] = []
    for src in config.EXTRA_SOURCES:
        try:
            if src.lower() == "medicaltech":
                articles = fetch_medicaltech(timeout=config.FETCH_TIMEOUT)
                extra_articles.extend(articles)
                logger.info("Fetched %d articles from medicaltech-news", len(articles))
            elif src.lower() == "htwatch":
                articles = fetch_htwatch(timeout=config.FETCH_TIMEOUT)
                extra_articles.extend(articles)
                logger.info("Fetched %d articles from ht-watch", len(articles))
            elif src.lower() == "googlenews" or src.lower() == "google-news":
                articles = fetch_google_news(timeout=config.FETCH_TIMEOUT)
                extra_articles.extend(articles)
                logger.info("Fetched %d articles from Google News", len(articles))
            else:
                logger.warning("Unknown extra source: %s", src)
        except Exception as exc:
            logger.exception("Failed to fetch from %s: %s", src, exc)
    
    if extra_articles:
        med_c, it_c, both_c = _count_keyword_hits(extra_articles)
        logger.info("Web scraping keyword hits (before exclude): med=%d it=%d both=%d", med_c, it_c, both_c)
        fetched.extend(extra_articles)
    
    if not fetched:
        logger.warning("ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    filtered = filter_articles(
        fetched,
        medical_keywords=config.MEDICAL_KEYWORDS,
        it_keywords=config.IT_KEYWORDS,
        exclude_keywords=config.EXCLUDE_KEYWORDS,
    )
    filtered = deduplicate_articles(filtered)
    filtered = sort_articles(filtered)

    # æ‰‹å‹•å®Ÿè¡Œæ™‚ã¯é€ä¿¡æ¸ˆã¿URLã®ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€Google Newsã‚’é™¤å¤–
    if manual:
        logger.info("Manual mode: skipping sent URL check, excluding Google News, limiting to %d articles", max_items)
        # Google Newsã‚’é™¤å¤–
        filtered_without_google = [a for a in filtered if a.link and "news.google.com" not in a.link]
        logger.info("Excluded Google News: %d articles remaining (from %d total)", len(filtered_without_google), len(filtered))
        new_articles = filtered_without_google
        if len(new_articles) > max_items:
            new_articles = new_articles[:max_items]
    else:
        # é€ä¿¡æ¸ˆã¿URLã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜æ—¥ã®åˆ¥ã®æ™‚é–“å¸¯ã§ã‚‚é‡è¤‡ã‚’é˜²ãï¼‰
        sent_map = load_sent_urls(storage_path)
        logger.info("Loaded %d sent URLs from storage", len(sent_map))
        
        new_articles = []
        skipped_count = 0
        for article in filtered:
            if not article.link:
                continue
            # URLã‚’æ­£è¦åŒ–ã—ã¦ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®äº’æ›æ€§ã®ãŸã‚ã€å…ƒã®URLã¨æ­£è¦åŒ–URLã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯ï¼‰
            normalized_url = article.link.rstrip("/").split("?")[0]
            if article.link in sent_map or normalized_url in sent_map:
                skipped_count += 1
                logger.debug("Skipping already sent article: %s", article.title[:50])
                continue
            new_articles.append(article)
        
        if skipped_count > 0:
            logger.info("Skipped %d already sent articles (from previous time slots)", skipped_count)
        
        if len(new_articles) > max_items:
            new_articles = new_articles[:max_items]

    now = datetime.now(timezone(timedelta(hours=9)))
    message = build_message(new_articles, now)

    logger.info("New articles: %d (after dedupe and limit)", len(new_articles))

    if dry_run:
        logger.info("Dry-run mode: message not sent to Slack.")
        print(message)
        return 0

    success = post_message(config.SLACK_WEBHOOK_URL, message, timeout=config.FETCH_TIMEOUT)
    if not success:
        logger.error("Slack é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return 1

    # é€ä¿¡æ¸ˆã¿URLã‚’ä¿å­˜ï¼ˆæ‰‹å‹•å®Ÿè¡Œæ™‚ã‚‚ä¿å­˜ã—ã¦ã€å®šæ™‚å®Ÿè¡Œæ™‚ã«é‡è¤‡ã‚’é˜²ãï¼‰
    if new_articles:
        sent_map = load_sent_urls(storage_path)  # æœ€æ–°ã®çŠ¶æ…‹ã‚’å†èª­ã¿è¾¼ã¿
        timestamp = now.isoformat()
        for article in new_articles:
            # URLã‚’æ­£è¦åŒ–ã—ã¦ä¿å­˜ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨åŒã˜å½¢å¼ã«ã™ã‚‹ï¼‰
            normalized_url = article.link.rstrip("/").split("?")[0]
            sent_map[normalized_url] = timestamp
        save_sent_urls(sent_map, storage_path)
        if manual:
            logger.info("Manual mode: saved %d sent URLs to %s (to prevent duplicate in scheduled runs)", len(new_articles), storage_path)
        else:
            logger.info("Saved %d sent URLs to %s", len(new_articles), storage_path)

    return 0


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="åŒ»ç™‚Ã—ITãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ã—ã¦Slackã«æŠ•ç¨¿ã—ã¾ã™ã€‚",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Slack ã«é€ã‚‰ãšæ¨™æº–å‡ºåŠ›ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã™ã‚‹",
    )
    parser.add_argument(
        "--storage-path",
        type=Path,
        help=f"é…ä¿¡æ¸ˆã¿URLä¿å­˜å…ˆ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {config.SENT_URLS_PATH})",
    )
    parser.add_argument(
        "--max-items",
        type=int,
        help=f"1å›ã®æŠ•ç¨¿ä»¶æ•°ä¸Šé™ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {config.MAX_ARTICLES_PER_POST})",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹ã«ã™ã‚‹",
    )
    parser.add_argument(
        "--manual",
        action="store_true",
        help="æ‰‹å‹•å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ï¼ˆé€ä¿¡æ¸ˆã¿URLãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€5ä»¶ã«åˆ¶é™ï¼‰",
    )
    return parser.parse_args()


def configure_logging(verbose: bool = False) -> None:
    level = logging.DEBUG if verbose else logging.INFO
    log_format = "%(asctime)s %(levelname)s %(name)s - %(message)s"
    if config.LOG_FILE:
        handlers = [
            logging.StreamHandler(),
            logging.FileHandler(config.LOG_FILE, encoding="utf-8"),
        ]
        logging.basicConfig(level=level, format=log_format, handlers=handlers)
    else:
        logging.basicConfig(level=level, format=log_format)


if __name__ == "__main__":
    args = parse_args()
    configure_logging(args.verbose)
    exit_code = run(
        dry_run=args.dry_run,
        storage_path=args.storage_path,
        max_items=args.max_items,
        manual=args.manual,
    )
    raise SystemExit(exit_code)
